{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02ae6f62",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- **Objective**:\n",
    "    - Feature engineering was necessary for previous models like Random Forest and XGBoost. To minimize feature engineering while effectively handling raw text, transformer models, particularly BERT, caught my attention. PyTorch provides pre-trained models like BERT that can be used without extensive feature engineering. These models capture context better and can incorporate static features like upvotes.\n",
    "\n",
    "- **Transformer Models**:\n",
    "    - Transformer models are highly effective for NLP tasks due to their ability to capture context and relationships within text. However, they are computationally intensive. To manage this, I adjusted the model from the basic BERT to a lighter version such as **distilbert-base-uncased**, which is more efficient.\n",
    "\n",
    "- **Tuning Parameters**:\n",
    "    To decrease the computational load, the following parameters can be tuned:\n",
    "\n",
    "    - **Max Sequence Length**: Reducing the maximum sequence length (max_length) decreases the number of tokens processed, thus reducing computation.\n",
    "    - **Batch Size**: Smaller batch sizes can help manage memory usage.\n",
    "    - **Learning Rate**: Adjusting the learning rate can impact the training time and model performance.\n",
    "    - **Number of Layers**: Using a lighter version of BERT, like DistilBERT, which has fewer layers, reduces computation.\n",
    "    - Training transformer models on a local machine without a powerful GPU is challenging. A GPU is essential for reasonable training times.\n",
    "\n",
    "- **Comparison with Tree-Based Models**:\n",
    "    - Due to computational limitations(**Apple M2, 16GB memory**), I couldn't directly compare the transformer model's performance with the previous tree-based models. However, I am confident that transformers would perform better with larger datasets and higher computational resources.\n",
    "\n",
    "The following code can be executed successfully with adequate computational resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5e99df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn, optim\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0486c584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0559a7d9",
   "metadata": {},
   "source": [
    "### Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bef7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode text data\n",
    "def encode_text(text, max_length=64):\n",
    "    encoded = tokenizer.encode_plus(\n",
    "        text,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return encoded['input_ids'].squeeze(0), encoded['attention_mask'].squeeze(0)\n",
    "\n",
    "# preprocess the dataset\n",
    "def preprocess_data(data):\n",
    "    data['encoded'] = data.apply(lambda row: encode_text(row['title'] + \" \" + (row['body'] if pd.notnull(row['body']) else '')), axis=1)\n",
    "    data['input_ids'], data['attention_mask'] = zip(*data['encoded'])\n",
    "    data['input_ids'] = list(data['input_ids'])\n",
    "    data['attention_mask'] = list(data['attention_mask'])\n",
    "    return data\n",
    "\n",
    "# Dataset class to handle input_ids, attention_mask, and scores\n",
    "class ScoreDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_mask, scores):\n",
    "        self.input_ids = torch.stack(input_ids) if not isinstance(input_ids[0], torch.Tensor) else input_ids\n",
    "        self.attention_mask = torch.stack(attention_mask) if not isinstance(attention_mask[0], torch.Tensor) else attention_mask\n",
    "        self.scores = torch.tensor(scores) if not isinstance(scores, torch.Tensor) else scores\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.scores)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.attention_mask[idx], self.scores[idx]\n",
    "\n",
    "# Define the model\n",
    "class BertRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertRegressor, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.regressor = nn.Linear(768, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        with torch.no_grad():\n",
    "            outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        score = self.regressor(pooled_output)\n",
    "        return score\n",
    "\n",
    "# Training loop\n",
    "def train(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for input_ids, attention_mask, scores in loader:\n",
    "        input_ids, attention_mask, scores = input_ids.to(device), attention_mask.to(device), scores.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(input_ids, attention_mask)\n",
    "        loss = criterion(predictions, scores.float().unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b112d5",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537f0e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('askscience_data.csv')\n",
    "data = preprocess_data(data)\n",
    "\n",
    "# Assume scores are stored in 'data['score']'\n",
    "dataset = ScoreDataset(data['input_ids'], data['attention_mask'], data['score'])\n",
    "loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Setup the device, model, loss function, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BertRegressor().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Training process\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    loss = train(model, loader, optimizer, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
